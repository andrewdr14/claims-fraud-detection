<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Motor Insurance Fraud Detection</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='styles.css') }}">
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f4f4f4;
            margin: 20px;
            padding: 20px;
        }
        h1, h2 {
            color: #333;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            background-color: #fff;
            box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
        }
        th, td {
            padding: 10px;
            text-align: center;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #007bff;
            color: white;
        }
        footer {
            margin-top: 20px;
            text-align: center;
            font-size: 12px;
            color: #777;
        }
        .intro {
            padding: 15px;
            background-color: #ffffff;
            border: 1px solid #ddd;
            box-shadow: 0px 0px 5px rgba(0, 0, 0, 0.1);
        }
        .subsection {
            margin-bottom: 32px;
        }
        .graph-row {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 18px;
            margin: 18px 0 24px 0;
        }
        .inline-img {
            display: block;
            max-width: 33vw;
            min-width: 300px;
            width: 100%;
            height: 340px;
            object-fit: contain;
            border: 1px solid #ccc;
            background: #fff;
            padding: 5px;
            border-radius: 6px;
            margin: 0;
        }
        .metric-table {
            margin-bottom: 30px;
        }
        @media (max-width: 1000px) {
            .graph-row {
                flex-direction: column;
                gap: 10px;
            }
            .inline-img {
                max-width: 100%;
                min-width: 0;
                height: 260px;
            }
        }
        .metric-details li {
            margin-bottom: 7px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Motor Insurance Fraud Detection</h1>
    </header>

    <section class="intro">
        <h2>üîç Project Overview</h2>
        <p>This tool is designed to analyze insurance claims and detect potential fraud using machine learning models. It compares the performance of <strong>Random Forest</strong> and <strong>XGBoost</strong> classifiers in identifying fraudulent claims based on structured data.</p>
    </section>

    <section>
        <h2>üïµÔ∏è How Fraudulent Claims Are Determined</h2>
        <div>
            <b>Fraud labels are assigned using a combination of business rules and randomness to simulate real-world error and noise:</b>
            <ul>
                <li>
                    <b>Rules:</b> A claim is marked as fraud if two or more of the following are true:
                    <ul>
                        <li>Total claim amount &gt; $15,000 and deductible is $500</li>
                        <li>Incident happens late at night or early morning (before 5am or after 10pm)</li>
                        <li>Collision type is "Hit & Run"</li>
                        <li>Insured is under 22 years old and claim amount &gt; $12,000</li>
                        <li>Number of vehicles &ge; 3 and annual premium &gt; $2,500</li>
                    </ul>
                </li>
                <li>
                    <b>Randomness:</b>
                    <ul>
                        <li>Even if rules are met, a small number of fraudulent claims are missed (simulated false negatives).</li>
                        <li>If only one rule is met, there‚Äôs a small chance of being marked as fraud (simulated false positives).</li>
                        <li>If no rules are met, there‚Äôs still a tiny chance of a random fraud label.</li>
                    </ul>
                </li>
                <li>This approach creates a realistic mix of correctly and incorrectly labeled fraud claims, making the detection task more challenging and authentic.</li>
            </ul>
        </div>
    </section>

    <section>
        <h2>üìã Data Used</h2>
        <h3>All Data Points Generated and Used for Model Training</h3>
        <ul>
          {% for col in all_columns %}
            <li><b>{{ col }}</b>: {{ data_descriptions.get(col, "No description available.") }}</li>
          {% endfor %}
        </ul>
    </section>

    <section>
        <h2>üìä Policy Holder Statistics</h2>
        <table>
            <tr>
                <th>Statistic</th>
                <th>Value</th>
            </tr>
            <tr>
                <td>Total Policy Holders</td>
                <td>{{ summary_stats["Total Policy Holders"] }}</td>
            </tr>
            <tr>
                <td>Fraud Reported</td>
                <td>{{ summary_stats["Fraud Reported"] }}</td>
            </tr>
            <tr>
                <td>No Fraud Reported</td>
                <td>{{ summary_stats["No Fraud Reported"] }}</td>
            </tr>
        </table>
    </section>

    <section>
        <h2>üìà Statistical Analysis</h2>
        <table>
            <tr>
                <th>Feature</th>
                <th>Min</th>
                <th>Max</th>
                <th>Median</th>
                <th>Mean</th>
                <th>Std Dev</th>
            </tr>
            {% for feature in summary_stats.keys() if "Min" in feature %}
            <tr>
                <td>{{ feature.split(" - ")[0] }}</td>
                <td>{{ summary_stats[feature] }}</td>
                <td>{{ summary_stats[feature.replace("Min", "Max")] }}</td>
                <td>{{ summary_stats[feature.replace("Min", "Median")] }}</td>
                <td>{{ summary_stats[feature.replace("Min", "Mean")] }}</td>
                <td>{{ summary_stats[feature.replace("Min", "Std Dev")] }}</td>
            </tr>
            {% endfor %}
        </table>
    </section>

    <section class="subsection">
        <h2>üõ° Random Forest Evaluation</h2>
        <table class="metric-table">
            <tr>
                <th>Metric</th>
                <th>Class 0 (No Fraud)</th>
                <th>Class 1 (Fraud)</th>
                <th>Macro Avg</th>
                <th>Weighted Avg</th>
            </tr>
            {% for metric in ["precision", "recall", "f1-score"] %}
            <tr>
                <td>{{ metric.capitalize() }}</td>
                <td>{{ "%.2f"|format(rf_results["0"][metric]) }}</td>
                <td>{{ "%.2f"|format(rf_results["1"][metric]) }}</td>
                <td>{{ "%.2f"|format(rf_results["macro avg"][metric]) }}</td>
                <td>{{ "%.2f"|format(rf_results["weighted avg"][metric]) }}</td>
            </tr>
            {% endfor %}
        </table>
        <p><b>ROC-AUC:</b> {{ "%.3f"|format(rf_roc_auc) }} &nbsp;|&nbsp; <b>PR-AUC:</b> {{ "%.3f"|format(rf_pr_auc) }}</p>
        <div class="graph-row">
            <img class="inline-img" src="data:image/png;base64,{{ rf_cm_img }}" alt="Random Forest Confusion Matrix" title="Random Forest Confusion Matrix">
            <img class="inline-img" src="data:image/png;base64,{{ rf_roc_img }}" alt="Random Forest ROC Curve" title="Random Forest ROC Curve">
            <img class="inline-img" src="data:image/png;base64,{{ rf_pr_img }}" alt="Random Forest Precision-Recall Curve" title="Random Forest Precision-Recall Curve">
        </div>
    </section>

    <section class="subsection">
        <h2>‚ö° XGBoost Evaluation</h2>
        <table class="metric-table">
            <tr>
                <th>Metric</th>
                <th>Class 0 (No Fraud)</th>
                <th>Class 1 (Fraud)</th>
                <th>Macro Avg</th>
                <th>Weighted Avg</th>
            </tr>
            {% for metric in ["precision", "recall", "f1-score"] %}
            <tr>
                <td>{{ metric.capitalize() }}</td>
                <td>{{ "%.2f"|format(xgb_results["0"][metric]) }}</td>
                <td>{{ "%.2f"|format(xgb_results["1"][metric]) }}</td>
                <td>{{ "%.2f"|format(xgb_results["macro avg"][metric]) }}</td>
                <td>{{ "%.2f"|format(xgb_results["weighted avg"][metric]) }}</td>
            </tr>
            {% endfor %}
        </table>
        <p><b>ROC-AUC:</b> {{ "%.3f"|format(xgb_roc_auc) }} &nbsp;|&nbsp; <b>PR-AUC:</b> {{ "%.3f"|format(xgb_pr_auc) }}</p>
        <div class="graph-row">
            <img class="inline-img" src="data:image/png;base64,{{ xgb_cm_img }}" alt="XGBoost Confusion Matrix" title="XGBoost Confusion Matrix">
            <img class="inline-img" src="data:image/png;base64,{{ xgb_roc_img }}" alt="XGBoost ROC Curve" title="XGBoost ROC Curve">
            <img class="inline-img" src="data:image/png;base64,{{ xgb_pr_img }}" alt="XGBoost Precision-Recall Curve" title="XGBoost Precision-Recall Curve">
        </div>
    </section>

    <section>
        <h2>‚ÑπÔ∏è About The Evaluation Metrics</h2>
        <div style="background:#fff;padding:18px 20px 10px 20px;border:1px solid #ddd;border-radius:6px;margin-bottom:20px;">
            <ul class="metric-details">
                <li><b>Precision:</b> The proportion of positive identifications (claims predicted as fraud) that were actually correct. High precision means fewer false positives. <br>
                    <i>Example: If the model says 100 claims are fraud and 90 really are, precision is 0.90.</i>
                </li>
                <li><b>Recall (Sensitivity):</b> The proportion of actual positive cases (true fraud) that were identified correctly. High recall means fewer false negatives.<br>
                    <i>Example: If there are 100 actual fraud cases and the model finds 80, recall is 0.80.</i>
                </li>
                <li><b>F1-Score:</b> The harmonic mean of precision and recall. It is a single metric that balances both, especially useful if your data is imbalanced.<br>
                    <i>F1 = 2 √ó (precision √ó recall) / (precision + recall).</i>
                </li>
                <li><b>Confusion Matrix:</b> A table showing how many claims were correctly/incorrectly classified as fraud or not fraud. <br>
                    <i>Rows = Actual class, Columns = Predicted class. Top left = True Negatives, top right = False Positives, bottom left = False Negatives, bottom right = True Positives.</i>
                </li>
                <li><b>ROC-AUC (Receiver Operating Characteristic - Area Under Curve):</b> Measures the model's ability to distinguish between classes across all thresholds.<br>
                    <i>The ROC curve plots True Positive Rate vs. False Positive Rate. The AUC summarizes performance: 1.0 = perfect, 0.5 = random guessing. Higher is better.</i>
                </li>
                <li><b>PR-AUC (Precision-Recall Area Under Curve):</b> Especially useful for imbalanced data. Shows the tradeoff between precision and recall for different thresholds.<br>
                    <i>Higher PR-AUC means the model is better at finding actual fraud with fewer false alarms, even when fraud is rare.</i>
                </li>
            </ul>
        </div>
    </section>

    <section>
        <h2>üì• Download Dataset</h2>
        <p>You can download the processed dataset below:</p>
        <a href="{{ url_for('download_data') }}" class="btn">Download CSV</a>
    </section>

</body>
</html>